

$Id$

I want a lot of flexibility setting and getting but without having a ton
of different ways to do it that are stupid.

Peter pointed out that SQL does it in different ways.  For example, with
inserts you have a list of fields being inserted and a list of the data,
but with update, it's key-value.

A lot of the time key-value is handier, but when bulk-loading data, I more
want to suck the stuff out of a list.


the way it works right now:

foo set $key field value field value field value...

foo get $key field field field field - returns a list of the values matching
the requested fields

foo get $key - returns a list of all of the values in field definition order


SETTING FOR THE FUTURE

The thing is, I want to import, like, a big file or something...

Then it'll be a list of data with a separate list of fields to import

So it could either be something like

foo import $key $fields $values

or

foo import $key $values field field field field field

or all fields if no fields are specified

Status:  Implemented

I might also want to import from an array

foo set $key -array arrayName  - do all of the fields in the array
 
foo set $key -array arrayName field field field field - do only the specified
fields

Set From Array Status: Not Implemented

Note: There is no easy way from a C extension to tell what the elements of
an array are without Eval'ing Tcl code, so we will probably require the
fields.  This can also easily be done by doing a

foo set key field1 $array(field1) field2 $array(field2) ...


GETTING FOR THE FUTURE

It will be real common to want to pull stuff into an array

foo get $key -array arrayName

Sometimes I won't want all the fields.  It will be much more efficient
when I don't need them all, to not convert them all back to Tcl.

foo get $key -array arrayName field field field field

I may also want to write out key-value pairs, not just values.

foo get $key -pairs field field field field

returns

key value key value key value

key-value fetch status:  Implemented



==========

Implementation

The problem is that set is pretty hard-coded, and its functionality needs
to be factored and made more general-purpose.

I think I need to factor out a routine that lets me pass a field name,
a pointer, and an object that contains the value to be set.

YES - with that, the rest becomes much easier.



========

INCREDIBLY FAST IMPORTING

What we want to do here is get a field name list and find the enumerated
field numbers in advance of looping on the import, so we only do the
lookup once.

something like

foo import code field field field field

code is executed repeatedly and is expected to return a list of the
proper length every time.

If it returns a break (TCL_BREAK), we stop.

Otherwise we take the first field as the key and each subsequent field
gets assigned in turn

if no fields are named, it expects them all to be there

Status: Implemented

========

For "foreach" and "names" methods, accept an optional argument that's
a Tcl_Match pattern.

Status:  Implemented


========

TRACKING FIELDS THAT CHANGE AND TAKING ACTIONS WHEN THINGS CHANGE

I have an interest in knowing that fields have changed.  If a field hasn't
changed, then it doesn't have to be included when, say, generating an
update back to a SQL database.

However, a CTable user may not always want this capability, so if we
implement this you will be able to specify whether you want a CTable you 
were generating to include code (and bit fields) for this or not.

When using this capability, a "dead band" should also be specifiable.  For
instance, we may be uninterested in recording a change in a floating point
field unless it has deviated by, say, more than 0.02 from its current
value.  This would mean that we would need to keep two values for numeric
fields, the last value emitted and the last value read, so that gets would
return the last value read but deadband checks would be against the last
value emitted (or if not emitted, the first value set), to guard against
any sort of deadband creep.

Finally, it could be useful to have some kind of trigger processing.  You
would define limits for a value and if a value was set within those various
limits, than pre-specified actions would be taken.  For example, alarms
would be generated.

In conjunction with the above deadband, or actually specifically related
to alarms, a deadband would prevent an alarm from "fibrillating".

For example, say there was an alarm generated when a disk drive hit
95% of its capacity, and due to activity on the disk, it was flipping between
94.99% and 95.01% of capacity several times a minute.  Rather than generating
numerous alarms and return-to-normals per minute, deadband processing might
require, for instance, that the drive usage drop to 94.0% before a return
to normal was issued.

This work has previously been implemented by Karl as Itcl alarm classes.

It would be acceptable for limit trips to cause Tcl code to be interpreted --
i.e. I don't think it would be necessary to try to avoid Tcl processing of
alarm processing code by supporting handling trigger processing directly in C,
although that isn't to say that we couldn't.

========

When loading a file, allow globbing of the key field as requested by Peter.

Status:  Implemented

========

QUERIES

Make a pass over all the rows, fetching requested variables into Tcl variables
and evaluating a Tcl expression on each one.  If the expression returns true,
save off a pointer to that row into an array of pointers equal in number of
elements to the number of elements in the table.

Also a select all will copy every row's pointer into the query result table.

SORT

Along with the query function, provide a way to sort on any field, or even
more than one field, by creating a sort comparison routine that functions at
the C level and feeding it to qsort_r.

C QUERIES

Queries implemented directly in C would be much faster than ones based on Tcl
but would basically have to either be predefined or generated, compiled and
linked as a shared library on the fly.  For a first implementation, only allow
them to be defined at compile time.  This ties into earlier proposed ways to
specify C code to be compiled, linked and loaded along with the autogenerated
ctable code.


=======

x select varList expression code

=======

Shared memory?

Super fast queries?

Pretty fast queries?

Sorting query results?

Threaded server?

----

In search have a select-into type thing where you're copying to another table,
possibly with different fields in them.

for search have -array_get varName -array_get_with_nulls varName and -get varName



C EXPRESSIONS
-------------

Add a way to declare and use C expressions where you can pass a pointer to
the table structure and the user-specified C expression, compiled into
C, gives you a select / skip.



Make varstringSetSource cache an allocated length and not free and malloc
new memory whenever a value is set.  Possibly even do a strncmp to see
if they're the same and do nothing.


TURBO MODE IN COMPARISONS
-------------------------

If they're only comparing one thing, we can potentially speed up certain
operations by shortcutting calling the search compare routine for the
field, depending on the operation.

For example, if it's just a null check or it's some check we do often
that we want to be extra fast, we handle it without calling the compare
routine.

INLINE
-----

Inline the search compare routines?  can't be done, they're called through
a pointer.

COMPARING FIELDS FROM ROW STRUCTURES INSTEAD OF GETTING FROM TCL

Sort compares do that already but search compares are hokey.  They get integers
from Tcl and shit like that.  Each time.  I've done some work to not do that
but it's kind of customed out.  The answer is to create a row that's not part
of the table for each comparison component and fill in the field or fields
in each that will be used.

You can't just share one special row among all the compares because you may
use a variable in more than one comparison and for "range".

-----

GENERATING SEPARATE COMPARE ROUTINES FOR EACH FIELD
---------------------------------------------------

right now all the compare routines are rolled up inside of a for loop that
walks the compare components of a search and has a big switch statement
for what field to alter and stuff.

if there was a separate compare routine for each field, there'd be less
overhead in comparing a single field.  If searching on a single field,
less interpreting the intentions of the caller of the search compare
routine.

The skiplist compare routine could match what the original skiplist code
expected -- super simple, two pointers to rows.

The compare routine would return -1, 0 or 1.

But we also allow searching for null, notnull, true, false, and match...

So do we generate a routine for each of those for each field?

Cooly, LT, LE, EQ, NE, GE and GT can all be sussed by looking at that -1, 0, 1.


SHOULD WE EVEN REQUIRE A KEY
----------------------------

The most basic level would be the "recno" thing.



SIMPLIFYING
-----------

Generate separate set routines for each field in a table.

Make null tracking optional.

Support indexes and/or triggers but in some superefficient way, especially
where there isn't an index/trigger on the field being set.

If set sets a new value into a field and that field hasn't got a trigger,
there needs to be minimal overhead.  If it does have a trigger and there's
a change, you gotta update the table.


SKIP LISTS ARE SO GOOD THEY NEED TO ALSO STAND ALONE
----------------------------------------------------

Need to implement skip list objects that take keys and values but the values
are just tcl objs, not ctable rows.  This gives you an associative array-like 
structure but one that can be walked in a foreach style method that will be
in sorted order.  This is powerful for where you're faking multidimensional arrays because with normal Tcl if you're doing a foo:* to wildcard everything with foo as the first element Tcl's going to do a full hashtable walk (and guess what, hashtable walks are slower than you'd think) whereas with this stuff it'll get out to foo:* really quickly and then read those elements in sequence.  If you had foo:bar:* it's still a win.  The only lose is when it's something like foo:*:bar where you're still going to have to walk foo:* to figure it out.  Even then it's still better than using hash tables.

...still haven't solved the hierarchy of storage issue to my satisfaction.  You know, in C you can go p->foo[4]->abc[i].snap, similarly in javascript, etc.
We need that kind of data capability in Tcl or we need to find a different language.

NAMESPACE OR DIRECTORY SUPPORT
------------------------------

You definitely want hierarchical namespaces for specifying the data.  In a complex app a flat structure either won't cut it or at least will be painful.  Tcl's namespaces can be leveraged like a tree-structured filesystem, whether the separators are :: or /.  Since we're already doing a ctable URL and we allow and parse the directory portion, although we don't yet use it, it seems pretty natural to map the directory onto namespaces.

MAKING CLIENT-SERVER GO FAST
----------------------------

The client/server stuff is amazing in the sense that it really does implement
a ctable object that works the same way but makes remote calls behind your
back for all of the actions.

However the server is single-threaded and ctables do not currently support shared memory.

Although this has not been a problem for flightaware, the row counts are not super large and the table is replicated widely from, essentially, a dual source.


