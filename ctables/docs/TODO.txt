$Id$

Create tests for shared memory ctables.

Make it work with UTF8 chars.

Make sure the C source file and the
shared library is newer than the scarfed file and that the CVS version
of gentable that the stuff was built with hasn't altered.  This way if
the build previously failed for some reason or the version of gentable
got updated, that we still regenerate the shared library.

[peter] I think this is done.

Have a way to find out if a field is null out-of-band, that is, some kind of
method called is_null or the like that will tell if you if a field or fields
is null.

[peter] What context is this in?  Something like:
	% table field is_null $key $field
	% table is_null $key $field

It's easy to make the null_value setting be local to an instance of the
table and not global to all tables of a specific type.  tbl_ptr is defined
in the big case statement -- 

[peter] I looked into this and it is hard. There's references to the null
	all through the code, including from routines that are not passed
	the ctable struct.

be able to import from an array, including only specific variables
NO - no easy way to get names of an array from C and anyway you can just
do a foo set $key field $array(field) field2 $array(field2), etc.

[peter] foo set $key -nocomplain [array get array]

Add a way for someone doing a CTable definition to specify additional
files to include.

[peter] What does this mean?
	Examples?

Add a way to write methods in C and add additional C code to the stuff
being generated, from the CTable definition.  This may also involve generating
a C include file that defines stuff that is otherwise defined in the
C source file we generate, so that you don't have to specify your C
code in some kind of quoted ctable definition.

Make set return a 1 if it created the row and a 0 if it already exists.
(Uh, not sure how valuable this actually is.)

[peter] This is kind of transaction-ish.
	I think it's worth doing, it's certainly easy.

Do something like GNU autoconf/configure or dig compiler switches, etc, out
of tclConfig.sh.

[peter] Partly done.

fixed strings allocate one more character than requested

provide a way for a ctable to provide a complete replayable definition
of itself.  Prefer to compile this into the C code so there's no way
it can get off from whatever files are in the vicinity.

[peter] Done: meta_table cextension

implement btree-style index option for fields with both unique and not-unique
styles.

[peter] obviated by skiplists?

Adopt TclX's test structure and make lots more tests.

[peter] what does this mean?

In the _comp routine for varstring, if the pointer is NULL use the
default value for the field.

[peter] Think this is done.

----

read_tabsep can fail but add a thing where you can specify -error proc and
if it's going to fail and the error proc has been specified, it'll call
forward to the error proc rather than blowing out.

If it gets an error back from the error proc, it blows out.

If it gets a continue, it skips that row and continues.

If it just returns, like TCL_OK, in the result object will be a row that
read_tabsep should use in place place of the original, which is error
checked and called forward again should the returned thing have an error.

If it gets another error on the same column, then it'll error out.

---

Add a bytearray object.

[peter] or use the tcl object type?
---

Perhaps limit strings to 64K bytes to use shorts for the length and allocated
length.

Make skip lists support enforcement of unique constraint as they originally did.

Write the compiler debug flag into the stuff we look at to see if anything got changed, requiring a recompile of the speedtable definitions.

[peter] I've done this, and added a number of other flags that change things.

----

There's a fairly nasty bug involving both updating a row via set, inserting
a row via set, and inserting rows via read_tabsep when index inserts can
fail because of a unique constraint check.

The problem occurs when there are one or more unique indexes on a table.

Let's say an index insert fails due to the value not being unique.  Currently
we return an error but we do nothing to take the row out of the linked list
that every row is a part of and we do nothing to remove the row from any
indexes where insertion has already been successful.

This will lead to a protection violation of some sort as we do not know
that the thing isn't in one of its indexes and we will attempt to remove
it at deletion time or whatever.

If it's a new insert and there's an index insertion failure, the row should
not be inserted at all, i.e. be deinserted from any indexes it was successfully
inserted into, i.e. the insert fails.

If it's a modify, that's tougher.  SQL standard would say the row should
be left unmodified.  We could do it that way by knowing how much we'd
done and undoing it, which about for sure includes keeping a copy of the
row prior to making any changes.

Alternatively we can only not perform (or undo) the field changes that cause a
constraint violation.

I also want to do the callout thing for errors real soon.

[peter] This would also happen on a typecheck from a row being set.

---

Also I want to add some new options to search/search+ such as

    -delete 1 -- delete matching rows

[peter] done

    -into tableName -- copy matching fields from matching rows into table

[peter] I wonder how hard it would be to make a ctable act like an input
	channel, so one ctable could write-tabsep to another.

---

Tcl's hash tables have a custom hash table capability where you supply 
pointers to a hash function, a key compare function, an allocator function 
and a free function so it's probably possible to have the hash keys be 
integers, doubles, inet, mac, etc, and even use the actual fields in the rows 
as the hash keys.

--

make delete_all_rows use the linked list [0] rather than walking the hash table.
make "names" method do the same.

[peter] Since walking the hash table is faster than walking the skiplist,
	this is probably not a good idea.

make a speed table driver package that has all the non-generated source
like searching, sorting, hash tables, skip lists, etc, into a shared
library such that they aren't completely compiled every time.

This will require solving the problem on Mac OS X where I can't link to
libpgtcl from my shared library.

Create that driver package as a full-fledged Tcl extension including
an autoconf/configure script so that it works not just on FreeBSD and
Mac OS X but Linux, Solaris, UNIX, etc.

--

Implement a straight "-array arrayName" in search where they don't have to
do "array set" on the key-value pairs returned by -array_get and 
-array_get_with_nulls.  When using -array, fields that are null should
explicitly be unset from the array, although it might be desirable to have
a switch to cause null fields to be set into the array with the default
value.

[peter] This is done.

--

possibly store the first values set for a varstring when a new row is
created in-band in a single malloc with an offset rather than a pointer or
a pointer into the allocated memory but keep track of that you can't free
it in the normal manner.  also that static area can be reused if a new
value is set in that is the same size or smaller.

also peter suggested using fields that are part of a varstring in a speedtable
row in the fields themselves if they're short enough, like 4 or 8 or
maybe 12 bytes.

[peter] I suspect the extra checks would make this slower.

--

See how hard it would be to put the hash table entry at the start of the row,
in the row definition, obviating the need for a pointer to the row in the
hash table entry and in conjunction with looking at the actual value in the
field in the row that's being used for the hash table, not have a pointer
to the key either.

DONE - hash entries are now integral to rows, not pointed to by them and
from them, etc.

--

Revisit how hashtables are expanded.  16X at a time is probably too many.
Adopt a thing like in indexes where you can say the expected number of
rows.  This should be on an instance-by-instance basis rather than on
the meta table.

--

There is a superfluous pointer in every bidirectionally linked list (one
for each row and one for each indexable fileld in a row) back to the head
which we need because our linked list routines are dumb.

Switch to better linked list routines and save a pointer for ever row plus
a pointer for every indexable field in a row.

[peter] Be careful here, making the list handling smarter may break shared
	lists.

--

FIXED 

About for sure a memory leak in search teardown when search keys are strings.

If we're going to call the delete routine, we're going to need to make sure
that hashEntry.key is NULL and that we check it before freeing it in the
delete routine.

----

Prepared searches

Provide a way to cache the entire search created by ctable_SetupSearch.

[peter] I wonder if the Tcl_Obj caching could be used here?

Within that, provide a way to change the values of the synthesized rows
that are part of the search components.

This will actually be easy -- you simply provide an interface to invoking
the *_set routine on the synthesized row being altered.

Probably save off searches as an executable Tcl command that accesses the
search struct through clientData and manipulates with subcommands (methods)
that manipulate the search components.

---

Eliminate the search->nRetrieveFields < 0 business that's in quite a few
places in ctable_serach.c by setting search->nRetrieveFields and
search->retrieveFields even if no fields were specified in the search
request, resulting in all fields.

---

a "batch set" capability.  Possibly a batch delete as well.  This could
just go into the client-server stuff.  Sending stuff to be read by
read_tabsep is pretty batchish anyway.

Maybe you send a batch and it's a list of commands and each command gets
executed and all the errors are accumulated and at the end you get back an
OK if everything was cool and either an error or a status that's a list of
all the commands that failed or partially failed, with the index of each 
command that failed and the reason and maybe even the full command in
the status return.

OK after some thinking, here's the plan.  It's not a batch set, it's a batch.
Batch doesn't mean it's a transaction, it's not.  It's simply a batch of
zero or more batchable commands.

Each command is executed in turn.  If it succeeds and returns no result,
we're done.  If it succeeds and returns a result, we accumulate into our
return value the index number of the command, a success status, and the
return value, in a way that facilitates array get, i.e. the key is the
row number and the value is everything else.  In there, there are key-value
pairs too, perhaps, like status and value.

If the command gets an error, its index and error return are recorded and
accumulated into the result list.

Finally, the result list is returned.  You do not get an error if there were
errors in the batched commands -- you have to examine the return to see
what the errors are.

You can get an error back from batch but only for argument errors to batch
or a malformed list.  Other than that, it will succeed, even if everything
you asked to happen in the batch failed.

DONE


---

Change the name of creatorTable in the ctable typedef and wherever referenced
to creator.  DONE

---

Change read_tabsep to take "-keys 0|1" and "-include_field_names 0|1"

[peter] DONE
	It's not a list of arguments like in search, so these are -nokeys
	and -with_field_names.

---


