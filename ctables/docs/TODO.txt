$Id$

Make it work with UTF8 chars.

Make sure the C source file and the
shared library is newer than the scarfed file and that the CVS version
of gentable that the stuff was built with hasn't altered.  This way if
the build previously failed for some reason or the version of gentable
got updated, that we still regenerate the shared library.

Have a way to find out if a field is null out-of-band, that is, some kind of
method called is_null or the like that will tell if you if a field or fields
is null.

It's easy to make the null_value setting be local to an instance of the
table and not global to all tables of a specific type.  tbl_ptr is defined
in the big case statement -- 

be able to import from an array, including only specific variables
NO - no easy way to get names of an array from C and anyway you can just
do a foo set $key field $array(field) field2 $array(field2), etc.

Add a way for someone doing a CTable definition to specify additional
files to include.

Add a way to write methods in C and add additional C code to the stuff
being generated, from the CTable definition.  This may also involve generating
a C include file that defines stuff that is otherwise defined in the
C source file we generate, so that you don't have to specify your C
code in some kind of quoted ctable definition.

Make set return a 1 if it created the row and a 0 if it already exists.
(Uh, not sure how valuable this actually is.)

Do something like GNU autoconf/configure or dig compiler switches, etc, out
of tclConfig.sh.

fixed strings allocate one more character than requested

provide a way for a ctable to provide a complete replayable definition
of itself.  Prefer to compile this into the C code so there's no way
it can get off from whatever files are in the vicinity.

implement btree-style index option for fields with both unique and not-unique
styles.

Adopt TclX's test structure and make lots more tests.

In the _comp routine for varstring, if the pointer is NULL use the
default value for the field.

-----


read_tabsep can fail but add a thing where you can specify -error proc and
if it's going to fail and the error proc has been specified, it'll call
forward to the error proc rather than blowing out.

If it gets an error back from the error proc, it blows out.

If it gets a continue, it skips that row and continues.

If it just returns, like TCL_OK, in the result object will be a row that
read_tabsep should use in place place of the original, which is error
checked and called forward again should the returned thing have an error.

If it gets another error on the same column, then it'll error out.

---

Add a bytearray object.

---

Perhaps limit strings to 64K bytes to use shorts for the length and allocated
length.

Make skip lists support enforcement of unique constraint as they originally did.

Write the compiler debug flag into the stuff we look at to see if anything got changed, requiring a recompile of the speedtable definitions.

----

There's a fairly nasty bug involving both updating a row via set, inserting
a row via set, and inserting rows via read_tabsep when index inserts can
fail because of a unique constraint check.

The problem occurs when there are one or more unique indexes on a table.

Let's say an index insert fails due to the value not being unique.  Currently
we return an error but we do nothing to take the row out of the linked list
that every row is a part of and we do nothing to remove the row from any
indexes where insertion has already been successful.

This will lead to a protection violation of some sort as we do not know
that the thing isn't in one of its indexes and we will attempt to remove
it at deletion time or whatever.

If it's a new insert and there's an index insertion failure, the row should
not be inserted at all, i.e. be deinserted from any indexes it was successfully
inserted into, i.e. the insert fails.

If it's a modify, that's tougher.  SQL standard would say the row should
be left unmodified.  We could do it that way by knowing how much we'd
done and undoing it, which about for sure includes keeping a copy of the
row prior to making any changes.

Alternatively we can only not perform (or undo) the field changes that cause a
constraint violation.

I also want to do the callout thing for errors real soon.

---

Also I want to add some new options to search/search+ such as

    -delete 1 -- delete matching rows

    -into tableName -- copy matching fields from matching rows into table

---

Tcl's hash tables have a custom hash table capability where you supply 
pointers to a hash function, a key compare function, an allocator function 
and a free function so it's probably possible to have the hash keys be 
integers, doubles, inet, mac, etc, and even use the actual fields in the rows 
as the hash keys.

--

make delete_all_rows use the linked list [0] rather than walking the hash table.
make "names" method do the same.

make a speed table driver package that has all the non-generated source
like searching, sorting, hash tables, skip lists, etc, into a shared
library such that they aren't completely compiled every time.

This will require solving the problem on Mac OS X where I can't link to
libpgtcl from my shared library.

Create that driver package as a full-fledged Tcl extension including
an autoconf/configure script so that it works not just on FreeBSD and
Mac OS X but Linux, Solaris, UNIX, etc.

--

Implement a straight "-array arrayName" in search where they don't have to
do "array set" on the key-value pairs returned by -array_get and 
-array_get_with_nulls.  When using -array, fields that are null should
explicitly be unset from the array, although it might be desirable to have
a switch to cause null fields to be set into the array with the default
value.

--

possibly store the first values set for a varstring when a new row is
created in-band in a single malloc with an offset rather than a pointer or
a pointer into the allocated memory but keep track of that you can't free
it in the normal manner.  also that static area can be reused if a new
value is set in that is the same size or smaller.

also peter suggested using fields that are part of a varstring in a speedtable
row in the fields themselves if they're short enough, like 4 or 8 or
maybe 12 bytes.

--

See how hard it would be to put the hash table entry at the start of the row,
in the row definition, obviating the need for a pointer to the row in the
hash table entry and in conjunction with looking at the actual value in the
field in the row that's being used for the hash table, not have a pointer
to the key either.

--

Revisit how hashtables are expanded.  16X at a time is probably too many.
Adopt a thing like in indexes where you can say the expected number of
rows.  This should be on an instance-by-instance basis rather than on
the meta table.

--

There is a superfluous pointer in every bidirectionally linked list (one
for each row and one for each indexable fileld in a row) back to the head
which we need because our linked list routines are dumb.

Switch to better linked list routines and save a pointer for ever row plus
a pointer for every indexable field in a row.


--

FIXED 

About for sure a memory leak in search teardown when search keys are strings.

If we're going to call the delete routine, we're going to need to make sure
that hashEntry.key is NULL and that we check it before freeing it in the
delete routine.


----

Prepared searches

Provide a way to cache the entire search created by ctable_SetupSearch.

Within that, provide a way to change the values of the synthesized rows
that are part of the search components.

This will actually be easy -- you simply provide an interface to invoking
the *_set routine on the synthesized row being altered.
