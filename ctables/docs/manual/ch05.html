<!DOCTYPE html PUBLIC "-//W3C//DTD HTML 4.01//EN" "http://www.w3.org/TR/html4/strict.dtd">
<html>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta http-equiv="Content-Style-Type" content="text/css">
  <title>Speed Tables - Methods</title>
  <link rel="stylesheet" href="manual.css">
</head>
<body>
<H2> 5 -  Methods for Manipulating Speed Tables</H2>
<div class="blue">This chapter enumerates all of the defined methods that are available to interact with Speed Tables, with examples.</div>
<H3>Meta-table Methods</H3>
<p>The following methods are available as arguments to the meta-table (class)
of the Speed Table:</p>
<dl>
<dt>create table_name ?master|reader ?name value?...?<dd>
<p>Create a new table instance. The syntax for master or reader shared memory tables are described in chapter 8</p>
<dt>cextension<dd>
<p>dump the code used to define the C Extension that created the table.</p>
<dt>info<dd><p>Reserved for future use</p>
<dt>method method_name method_proc<dd>
<p>Register a new method into the class. When the table is called with the
named method, the arguments are passed to the named proc. The proc will
be invoked with the table as its first argument, followed by any method
arguments.
<dt>null_value ?value?<dd>
<p>Define or fetch the null value for a class.</p>
</dl>
<H3> Table Methods </H3>
<p>The following built-in methods are available as arguments to each instance of a speed table:</p>
<p><i>get</i>, <i>set</i>, <i>array_get</i>, <i>array_get_with_nulls</i>, <i>exists</i>, <i>delete</i>, <i>count</i>, <i>foreach</i>, <i>type</i>, <i>import</i>, <i>import_postgres_result</i>, <i>export</i>, <i>fields</i>, <i>fieldtype</i>, <i>needs_quoting</i>, <i>names</i>, <i>reset</i>, <i>destroy</i>, <i>statistics</i>, <i>write_tabsep</i>, <i>read_tabsep</i>, <i>key</i>, <i>makekey</i>, <i>store</i>, <i>share</i>, <i>getprop</i>, <i>attach</i></p>
<p>For the examples, assume we have done a "<tt>cable_info create x</tt>"</p>
<dl>
<dt><i>set</i><dd>
<pre>
x set key ?-nocomplain? field value ?field value...?
</pre>
<p>or</p>
<pre>
x set key ?-nocomplain? keyValueList 
</pre>
<p>The key is required and it must be unique. It can contain anything you want. It's not also an element of the table.</p>
<p>We may change this in the future to make it possible to have tables that do not require any keys (there is already a provision for this, though incomplete) and also to allow more than one key. But for now, lame or not, this is how it works, and as Peter says, for more than one key, you can always create some kind of compound key.</p>
<pre>
<span> % </span>x set peter ip 127.0.0.1 name "Peter da Silva" i 501</span><span class="s9"> 
</pre>
<p>In the above example, we create a row in the <b>cable_info</b> table named "x" with an index of "peter", an ip value of 127.0.0.1, a name of "Peter da Silva", and an "i" value of 501. All fields in the row that have not been set will be marked as null. (Also any field set with the null value will also be marked as null.)</p>
<pre>
<span class="s10">% </span>set values [list ip 127.0.0.1 name "Peter da Silva" i 501]
<span class="s10">% </span>x set peter $values
</pre>
<p>In this example, we specify the value as a list of key-value pairs. This is a natural way to pull an array into a speed table row:</p>
<pre>
<span class="s10">% </span>x set key [array get dataArray]
</pre>
<p>By default it is an error to attempt to set a field in a row
that does not exist. However, if <tt>-nocomplain</tt> is specified,
such errors are suppressed, hence all matching fields are set and
any keys that do not exist in the table are silently ignored. This
is useful when an array contains some fields that you want to store
in a speedtable row but may contain additional fields that you do
not want to store but which, without <tt>-nocomplain</tt>, you'd
have to remove from the array prior to invoking <tt>set</tt>.</p>
<dt><i>store</i><dd>
<pre>
x store ?-nocomplain? field value ?field value?
</pre>
<p>or</p>
<pre>
x store ?-nocomplain? keyValueList
</pre>
<p>Store is similar to "set", but extracts the key from the provided fields. If the table does not have a field explicitly designated as a key, then the pseudo-field "_key" is used. If the key is not present in the list, then the next autogenerated value (see read_tabsep) will be used.</p>
<p>Store returns the key used to store the list.</p>
<dt><i>makekey</i><dd>
<pre>
x makekey field value ?field value?
</pre>
<p>or</p>
<pre>
x makekey keyValueList
</pre>
<p>This simply calculates what the appropriate key value for the list would be.</p>
<p>For example, for a table where the field "ip" was a key:</p>
<pre>
x makekey {ip 10.2.3.1 name host1}
</pre>
<p>would return "10.2.3.1"</p>
<dt><i>key</i><dd>
<p>Returns the name of the key field specified for the table, or "_key" if none were specified.</p>
<dt><i>fields</i><dd>
<p>"fields" returns a list of defined fields, in the order they were defined.</p>
<pre>
<span>% </span>x fields
<b>ip mac name address addressNumber geos i j ij</b>
</pre>
<dt><i>field</i><dd>
<p>"field" returns information about the values that defined the field. You can use this command to retrieve all the key-value pairs that define a field.</p>
<p>Since we accept (and ignore) arguments to field definitions for keys we don't recognize, you can define your own key-value pairs in field definitions inside of speed table definitions and access them using this method.</p>
<p>Following the name of the field should be one of the keywords <tt>getprop</tt>, <tt>properties</tt>, or <tt>proplist</tt>. <tt>properties</tt> will return the names of all of the properties as a Tcl list. <tt>proplist</tt> will return the names and values of all the properties as a Tcl list, in what we would call "array set" format. <tt>getprop</tt> will return the value associated with the key passed as an argument.</p>
<pre>
<span>% </span>$ctable field $fieldName proplist
<b>default 1 name alive type boolean</b>
<span>% </span>$ctable field $fieldName properties
<b>default name type</b>
<span>% </span>$ctable field $fieldName getprop default
</pre>
<dt><i>get</i><dd>
<p>Get fields. Get specified fields, or all fields if none are specified, returning them as a Tcl list.</p>
<pre>
% x get peter
<b>127.0.0.1 {} {Peter da Silva} {} {} {} 501 {} {}</b>
% x get peter ip name
<b>127.0.0.1 {Peter da Silva}</b>
</pre>
<dt><i>array_get</i><dd>
<p>Get specified fields, or all fields if none are specified, in "array get" (key-value pair) format. Note that if a field is null, it will not be fetched.</p>
<pre>
<span>% </span>x array_get peter
<b>ip 127.0.0.1 name {Peter da Silva} i 501</b>
<span>% </span>x array_get peter ip name mac
<b>ip 127.0.0.1 name {Peter da Silva}</b>
</pre>
<dt><i>array_get_with_nulls</i><dd>
<p>Get specified fields, or all fields if none are specified, in "array get" (key-value pair) format. If a field contains the null value, it is fetched anyway. (Yes this should probably be an option switch to array_get instead of its own method.)</p>
<pre>
% x array_get_with_nulls peter
<b>ip 127.0.0.1 mac {} name {Peter da Silva} address {} addressNumber </b>...
% x array_get_with_nulls peter ip name mac
<b>ip 127.0.0.1 name {Peter da Silva} mac {}</b>
</pre>
<p>Note that if the null value has been set, that value will be returned other than the default null value of an empty Tcl object.</p>
<pre>
<span>% </span>cable_info null_value \\N
<span>% </span>x array_get_with_nulls peter
<b>ip 127.0.0.1 mac \N name {Peter da Silva} address \N addressNumber </b>...
<span>% </span>x array_get_with_nulls peter ip name mac
<b>ip 127.0.0.1 name {Peter da Silva} mac \N</b>
</pre>
<dt><i>exists</i><dd>
<p>Return 1 if the specified key exists, 0 otherwise.</p>
<pre>
<span>% </span>x exists peter
<b>1</b>
<span>% </span>x exists karl
<b>0</b>
</pre>
<dt><i>delete</i><dd>
<p>Delete the specified row from the table. Returns 1 if the row existed, 0 if it did not.</p>
<pre>
<span>% </span>x delete karl
<b>0</b>
<span>% </span>x set karl
<span>% </span>x delete karl
<b>1</b>
<span>% </span>x delete karl
<b>0</b>
</pre>
<dt><i>count</i><dd>
<p>Return a count the number of rows in the table.</p>
<pre>
<span>% </span>x count
<b>1</b>
</pre>
<dt><i>batch</i><dd>
<p>Take a list of speed table commands (minus the table name, as that's implicit), and invoke each element of the list as a method invocation on the current speed table.</p>
<p>A result list is constructed.</p>
<p>As each command within the batch is invoked, if the invocation is successful and no value is returned, nothing is added to the result list.</p>
<p>If the invocation is successful and a value is returned, a list is added to the result list containing two elements: the number of the element of the batch list and a sublist containing the Tcl result code (0) and whatever the result was that was returned.</p>
<p>If the invocation failed, a list is added to the result list, containing the element index, as above, but with the Tcl result code set to TCL_ERROR (1) and the result portion is the error message returned.</p>
<pre>
% x batch {{set dean age 17} {incr dean age 1} {incr brock age foo}}
<b>{{1 {0 18}} {2 {1 {expected integer but got "foo" while converting age</b> ...
</pre>
<p>In this example, setting Dean's age to 17 produced no result. Incrementing it returned the incremented value (18), and trying to set Brock's age to a non-integer value recorded an error.</p>
<p>Note that errors in batched commands do not cause batch to return an error. It is up to the caller to examine the result of the batch command to see what happened.</p>
<p>"batch" will return an error in the event of bad arguments passed to it, the batch list being unparseable as a list, etc.</p>
<dt><i>search</i><dd>
<p>Search for matching rows and take actions on them, with optional sorting. Search exploits indexes on fields when available, or performs a brute force search if there are no indexed fields available in the compare list. These indexes are implemented using skip lists.</p>
<div class="blue-indent">Brute-Force Search Is Brutally Fast</div>
<div class="blue-block">
<p>Search can perform brute-force multivariable searches on a speed table and take actions on matching records, without any scripting code running on an every-row basis.</p>
<p>On a modern 2006 Intel and AMD machines, speed table search can perform, for example, unanchored string match searches at a rate of sixteen million rows per CPU second (around 60 nanoseconds per row).</p>
<p>On the other hand, skip lists point to a future where there isn't any key that's external to the row -- that is, what would have been the external key would exist as a normal field in the row.</p>
<p>Whether you should use indexes (skiplists) or not depends on the characteristics of the table. On one of our test systems, inserting a row into the table takes about 2.3 microseconds, but a single index increases this to about 7 microseconds. On the other hand, an indexed search on that field may be O(logN) on the number of rows in the table.</p>
</div>
<p>Search is a powerful element of the speed tables tool that can be leveraged to do a number of the things traditionally done with database systems that incur much more overhead.</p>
<pre>
$table search \
    ?-sort {?-?field..}? ?-fields fieldList? ?-glob pattern? \
    ?-compare list? ?-offset offset? ?-limit limit? \
    ?-code codeBody? ?-key keyVar? ?-get varName? \
    ?-array_get varName? ?-array_get_with_nulls varName? \
    ?-write_tabsep channel? ?-tab string? ?-with_field_names 0|1?
</pre>
<p>Search options:</p>
<dl>

<dt>-sort sortArg<dd>
<p>Sort results based on the specified field or fields. If multiple fields are specified, their precedence is in descending order. In other words, the first field is the primary search key.</p>
<p>If you want to sort a field in descending order, put a dash in front of the field name.</p>
<p class="bug">Bug: Speed tables are currently hard-coded to sort null values "high". As this is not always what one wants, an ability to specify whether nulls are to sort high or low will likely be added in the future.</p>
<dt>-fields fieldList<dd>
<p>Restrict search results to the specified fields.</p>
<p>If you have a lot of fields in your table and only need a few, using -fields to restrict retrieval to the specified fields will provide a nice performance boost.</p>
<p>Fields that are used for sorting and/or for comparison expressions do not need to be included in -fields in order to be examined.</p>
<dt>-glob pattern<dd>
<p>Perform a glob-style comparison on the key, excluding the examination of rows not matching.</p>
<dt>-offset offset<dd>
<p>If specified, begins actions on search results at the "offset" row found. For example, if offset is 100, the first 100 matching records are bypassed before the search action begins to be taken on matching rows.</p>
<dt>-limit limit<dd>
<p>If specified, limits the number of rows matched to "limit".</p>
<p>Even if used with -countOnly, -limit still works, so if, for example, you want to know if there are at least 10 matching records in the table but you don't care what they contain or if there are more than that many, you can search with -countOnly 1 -limit 10 and it will return 10 if there are ten or more matching rows.</p>
<dt>-write_tabsep channel<dd>
<p>Matching rows are written tab-separated to the file or socket (or postgresql database handle) "channel".</p>
<dt>-tab string<dd>
<p>Specify the separator string for write_tabsep (default "\t").</p>
<dt>-with_field_names 1<dd>
<p>If you are doing -write_tabsep, <tt>-with_field_names 1</tt> will cause the first line emitted to be a tab-separated list of field names.</p>
<dt>-key keyVar<dd>
<dt>-get listVar<dd>
<dt>-array arrayName<dd>
<dt>-array_with_nulls arrayName<dd>
<dt>-array_get listVar<dd>
<dt>-array_get_with_nulls listVar<dd>
<dt>-code codeBody<dd>
<p>Run scripting code on matching rows.</p>
<p>If <tt>-key</tt> is specified, the key value of each matching row is written into the variable specified as the argument that follows it.</p>
<p>If <tt>-get</tt> is specified, the fields of the matching row are written into the variable specified as the argument to -get. If <tt>-fields</tt> is specified, you get those fields in the same order. If <tt>-fields</tt> is not specified, you get all the fields in the order they were defined. If you have any question about the order of the fields, just ask the speed table with <tt>$table fields</tt>.</p>
<p><tt>-array_get</tt> works like <tt>-get</tt> except that the field names and field values are written into the specified variable as a list, in a manner that <i>array get</i> can load into an array. I call this "array set" format. Fields that are null are not retrieved with -array_get.</p>
<p><tt>-array_get_with_nulls</tt> pulls all the fields, substituting the null value (by default, an empty string) for any fields that are null.</p>
<p>Note it is a common bug to use <tt>-array_get</tt> in a <tt>-code</tt> loop, array set the returned list of key-value pairs into an array, and not <i>unset</i> the array before resuming the loop, resulting in null variables not being unset -- that is, from a previous row match, field x had a value, and in the current row, it doesn't.</p>
<p>If you haven't unset your array, and you "array get" the new result into the array, the previous value of x will still be there. So either unset (-nocomplain is a useful, not widely known optional argument to unset) or use array_get_with_nulls.</p>
<p>Better yet would be to just use -array or -array_with_nulls, both of which directly put the stuff in an array on your behalf and do the right thing with respect to null values.</p>
<p><tt>-array</tt> sets field names and field values into the named array. Any fields that are null are specifically removed (unset) from the array.</p>
<p>Thus, if you use -array to and you with to access a field can be null, you need to check to see if the field exists (using [info exists array(fieldName)], etc) before trying to look at its value.</p>
<p>If you don't want to do that, consider using -array_with_nulls instead.</p>
<p><tt>-array_with_nulls</tt> sets field names and field values into the named array. Any fields that are null are set into the array as the null value (by default, an empty string), as set by the <i>null_value</i> method of the creator table.</p>
<dt>-compare list<dd>
<p>Perform a comparison to select rows.</p>
<p>Compare expressions are specified as a list of lists. Each list consists of an operator and one or more arguments.</p>
<p>When the search is being performed, for each row all of the expressions are evaluated left to right and form a logical "and". That is, if any of the expressions fail, the row is skipped.</p>
<p>Here's an example:</p>
<pre>
$table search -compare {{&gt; coolness 50} \
{&gt; hipness 50}} ...
</pre>
<p>In this case you're selecting every row where coolness is greater than 50 and hipness is greater than 50.</p>
<p>Here are the available expressions:</p>
<dl>
<dt>{false field}<dd>
<p>Expression compares true if field's value is false. (For booleans, false. For shorts, ints and wides, false is 0 and anything else is true.</p>
<dt>{true field}<dd>
<p>Expression compares true if field is true.</p>
<dt>{null field}<dd>
<p>Expression compares true if field is null.</p>
<dt>{notnull field}<dd>
<p>Expression compares true if field is not null.</p>
<dt>{&lt; field value}<dd>
<p>Expression compares true if field less than value. This works with both strings and numbers, and yes, compares the numbers as numbers and not strings.</p>
<dt>{&lt;= field value}<dd>
<p>Expression compares true if field is less than or equal to value.</p>
<dt>{= field value}<dd>
<p>Expression compares true if field is equal to value.</p>
<dt>{!= field value}<dd>
<p>Expression compares true if field is not equal to value.</p>
<dt>{&gt;= field value}<dd>
<p>Expression compares true if field is greater than or equal to value.</p>
<dt>{&gt; field value}<dd>
<p>Expression compares true if field is greater than value.</p>
<dt>{match field expression}<dd>
<p>Expression compares true if field matches glob expression. Case is insensitive.</p>
<dt>{match_case field expression}<dd>
<p>Expression compares true if field matches glob expression, case-sensitive.</p>
<dt>{notmatch field expression}<dd>
<p>Expression compares true if field does not match glob expression. Case is insensitive.</p>
<dt>{notmatch_case field expression}<dd>
<p>Expression compares true if field does not match glob expression, case-sensitive.</p>
<dt>{range field low hi}<dd>
<p>Expression compares true if field is within the range of low &lt;= field &lt; hi.</p>
<dt>{in field valueList}<dd>
<p>Expression compares true if the field's value appears in the value list.  </p>
<p>The "in" search expression, when used as the first search term on an indexed field has very high performance, in particular with client-server ctables, as it is much faster to go find many rows in one query than to repeatedly cause a TCP/IP command/response roundtrip on a per-row basis.</p>
</dl>
<dt>-buffer 1<dd>
<p>Modifying the index being followed from the code body of a search can have undefined behavior. You may miss entries, visit entries twice, or possibly even crash the program. to avoid this, you can use a buffered search. The <tt>-buffer</tt> option transactionalizes the operation, at the cost of allocating a temporary internal array for the matched rows.</p>
<p>The <tt>-delete</tt> and <tt>-update</tt> options are automatically transactionalized when necessary. The <tt>-sort</tt> option uses the transaction buffer for sorting, so there is no additional overhead for <tt>-buffer</tt> on a sorted search unless the sort is optimized out by the query optimizer. Searches on shared memory tables are always buffered.</p>
<p>Note: It is not safe to delete rows from the code body of a search, whether buffered or not. Use the <tt>-delete</tt> option instead.</p>
<p class=bug>Bug: The table should lock out deletes during a search.</p>
<p class=bug>Bug: Indexes should be checked and the indexed field should be locked, so that modifying it unbuffered is defined as an error.</p>
<dt>-delete 1<dd>
<p>Delete the matched records.</p>
<dt>-update {field value}<dd>
<p>Update the named field to a new value.</p>
<p>
<dt>-poll_interval interval<dd>
<p>Perform an <tt>update</tt> operation every <tt>interval</tt> rows, to allow background processing to work in the background while a long search is going on.</p>
<dt>-poll_code {code}<dd>
<p>Perform the specified <tt>code</tt> every <tt>-poll_interval</tt> rows. Errors from the code will be handled by the </tt>bgerror</tt> mechanism. If no poll interval is specified then a default (1024) is used.</p>

</dl>

<p>Search Examples:</p>
<p>Write everything in the table tab-separated to channel $channel</p>
<pre>
$table search -write_tabsep $channel
</pre>
<p>Write everything in the table with coolness &gt; 50 and hipness &gt; 50:</p>
<pre>
$table search -write_tabsep $channel \
  -compare {{&gt; coolness 50} {&gt; hipness 50}}
</pre>
<p>Create some code to populate a Tcl array for every record in the table matching above:</p>
<pre>
set fp [open oldschool.tcl w]
$table search \
  -compare {{&gt; coolness 50} {&gt; hipness 50}} \
  -key key -array_get data -code {
      puts $fp [concat [list array set array($key)] $data]
  }
close $fp
</pre>

<dt><i>search+</i><dd>
<p>Search+ is a (deprecated) synonym for search. </p>
<dt><i>foreach</i><dd>
<p>DEPRECATED (use "search" instead)</p>
<pre>
x foreach varName ?pattern? codeBody
</pre>
<p>Iterate over all of the rows in the table, or just the rows in the table matching a string match wildcard, executing tcl code on each of them.</p>
<p>Example:</p>
<pre>
% x foreach key {
 puts $key
 }
</pre>
<p>This is equivalent to:</p>
<pre>
% x search -key key -code {
 puts $key
}
</pre>
<dt><i>incr</i><dd>
<p>Increment the specified numeric values, returning a list of the new incremented values</p>
<pre>
% x incr $key a 4 b 5
</pre>
<p>...will increment $key's a field by 4 and b field by 5, returning a list containing the new incremented values of a and b.</p>
<dt><i>type</i><dd>
<p>Return the "type" of the object, i.e. the name of the object-creating command that created it.</p>
<pre>
% x type
<b>cable_info</b>
</pre>
<dt><i>import_postgres_result</i><dd>
<pre>
x import_postgres_result handle ?-nokeys? ?-nocomplain?
</pre>
<p>Given a <i>Pgtcl</i> result handle, <i>import_postgresql_result</i> will iterate over all of the result rows and create corresponding rows in the table, matching the SQL column names to the field names.</p>
<p>If the "-nocomplain" option is specified unknown columns in the result will be ignored.</p>
<p>If the "-nokeys" option is specified the key is derived from the key column specified for the table, or autogenerated as described in <i>read_tabsep</i>.</p>
<p>This is extremely fast as it does not do any intermediate Tcl evaluation on a per-row basis.</p>
<p>The result handle can come from any Pgtcl source, such as <tt>pg_exec</tt>. You use pg_result to check if the request was successful.</p>
<pre>
set res [pg_exec $connection "select * from mytable"]
if {[pg_result $res -status] != "PGRES_RESULT_OK"} {... error handling ...}
x import_postgres_result $res
</pre>
<div class="blue-indent">Importing PostgreSQL Results Is Pretty Fast</div>
<p class="blue-block">On a 2 GHz AMD64 we are able to import about 200,000 10-element rows per CPU second, i.e. around 5 microseconds per row. Importing goes more slowly if one or more fields of the speed table has had an index created for it.</p>
<dt><i>fieldtype</i><dd>
<p>Return the datatype of the named field.</p>
<pre>
foreach field [x fields] {
 puts "$field type is [x fieldtype $field]"
 }
<b>ip type is inet</b>
<b>mac type is mac</b>
<b>name type is varstring</b>
<b>address type is varstring</b>
<b>addressNumber type is varstring</b>
<b>geos type is varstring</b>
<b>i type is int</b>
<b>j type is int</b>
<b>ij type is long</b>
</pre>
<dt><i>needs_quoting</i><dd>
<p>Given a field name, return 1 if it might need quoting. For example, varstrings and strings may need quoting as they can contain any characters, while integers, floats, IP addresses, MAC addresses, etc, do not, as their contents are predictable and their input routines do not accept tabs.</p>
<dt><i>names</i><dd>
<p>Return a list of all of the keys in the table. This is fine for small tables but can be inefficient for large tables as it generates a list containing each key, so a 650K table will generate a list containing 650K elements -- in such a case we recommend that you use <i>search</i> instead.</p>
<p>This should probably be deprecated.</p>
<dt><i>reset</i><dd>
<p>Clear everything out of the table. This deletes all of the rows in the table, freeing all memory allocated for the rows, the rows' hashtable entries, etc.</p>
<pre>
% x count
<b>652343</b>
% x reset
% x count
<b>0</b>
</pre>
<dt><i>destroy</i><dd>
<p>Delete all the rows in the table, free all of the memory, and destroy the object.</p>
<pre>
% x destroy
% x asdf
<b> invalid command name "x"</b>
</pre>
<dt><i>share</i><dd>
<dt><i>getprop</i><dd>
<p> Access information about the underlying shared memory associated with a shared memory table (see the secion on shared memory, below).</p>
<dt><i>attach</i><dd>
<p> Create an attachment for a shared reader table in a shared master table. Returns a set of <i>create</i> parameters to use to complete the attachment.</p>
<dt><i>statistics</i><dd>
<p> Report information about the hash table such as the number of entries, number of buckets, bucket utilization, etc. It's fairly useless, but can give you a sense that the hash table code is pretty good.</p>
<pre>
% x statistics
<b>1000000 entries in table, 1048576 buckets</b>
<b>number of buckets with 0 entries: 407387</b>
<b>number of buckets with 1 entries: 381489</b>
<b>number of buckets with 2 entries: 182642</b>
<b>number of buckets with 3 entries: 59092</b>
<b>number of buckets with 4 entries: 14490</b>
<b>number of buckets with 5 entries: 2944</b>
<b>number of buckets with 6 entries: 462</b>
<b>number of buckets with 7 entries: 63</b>
<b>number of buckets with 8 entries: 6</b>
<b>number of buckets with 9 entries: 0</b>
<b>number of buckets with 10 or more entries: 1</b>
<b>average search distance for entry: 1.5</b>
</pre>
<dt><i>write_tabsep  </i><dd>
<p>DEPRECATED (use search -write_tabsep)</p>
<pre>
x write_tabsep channel ?-glob pattern? ?-nokeys? ?-with_field_names? \
    ?-tab string? ?field...?
</pre>
<p>Write the table tab-separated to a channel, with the names of desired fields specified, else all fields if none are specified.</p>
<pre>
set fp [open /tmp/output.tsv w]
x write_tabsep $fp
close $fp
</pre>
<p>If the glob pattern is specified and the key of a row does not match the glob pattern, the row is not written.</p>
<p>The first field written will be the key, unless -nokeys is specified and the key value is not written to the destination.</p>
<p>If -with_field_names is specified, then the names of the fields will be the first row output.</p>
<p>If -tab is specified then the string provided will be used as the tab.</p>
<p class="bug">Bug: We do not currently quote any tabs that occur in the data, so if there are tab characters in any of the strings in a row, that row will not be read back in properly. In fact, we will generate an error when attempting to read such a row. In most cases it should be possible to select a tab separator that does not occur in any field to avoid this.</p>
<dt><i>read_tabsep</i><dd>
<pre>
x read_tabsep channel ?-glob pattern? ?-nokeys? ?-with_field_names? \
    ?-tab string? ?-skip pattern? ?-term pattern? ?field...?
</pre>
<p>Read tab-separated entries from a channel, with a list of fields specified, or all fields if none are specified.</p>
<pre>
set fp [open /tmp/output.tsv r]
x read_tabsep $fp
close $fp
</pre>
<p>The first field is expected to be the key (unless -nokeys is specified) and is not included in the list of fields. So if you name five fields, for example, each row in the input file (or socket or whatever) should contain six elements.</p>
<p>It's an error if the number of fields read doesn't match the number expected.</p>
<p>If the <tt>-glob pattern</tt> is defined, it's applied to the key (first field in the row) and if it doesn't match, the row is not inserted.</p>
<p>If <tt>-tab string</tt> is specified, then the string provided will be used as the tab separator. There is no explicit limit on the length of the string, so you can use something like <tt>-tab {%JULIE@ANDREWS%}</tt> with <i>read_tabsep</i> and <i>write_tabsep</i> (or <i>search -write_tabsep</i>) to reduce the possibility of a conflict.</p>
<p>if <tt>-skip pattern</tt> is specified, then lines matching that pattern are ignored. This is sometimes necessary for files containing comments.</p>
<p>If <tt>-with_field_names</tt> is specified, the first row read is expected to be a tab-separated list of field names, and Speedtables will read that line and use its contents to determine which fields each of the following lines of tab-separated values will be stored as. (This is the counterpart to the <tt>-with_field_names<tt> argument to speedtables's <i>search</i> method when invoked with the </tt>-write_tabsep</tt> option.)</p>
<p>If <tt>-nokeys</tt> is specified, the first field of each row is not used as the key -- rather, the key is taken from the provided fields (as if <i>makekey</i> was called for each row), and if there is no key it is automatically created as an ascending integer starting from 0. The last key generated will be returned as the value of <i>read_tabsep</i>.</p>
<p>If you subsequently do another <i>read_tabsep</i> with -nokeys specified, the auto key will continue from where it left off. If you invoke the table's reset method, the auto key will reset to zero.</p>
<p>If you later want to insert at the end of the table, you need to use <i>store</i> rather than <i>set</i>.</p>
<p><i>read_tabsep</i> stops when it reaches end of file OR when it reads an empty line.
The nice thing about it is you can indicate end of input with an empty line and then do something else with the data that follows.
Since you must have a key and at least one field, this is safe. However it might not be safe with <tt>-nokeys</tt>.</p>
<p>With <tt>-term pattern</tt> then read_tabsep will instead terminate on a line that matches <tt>pattern</tt>. For example, to emulate SQL "COPY FROM stdin" you could use <tt>-term "\\."</tt>.</p>
<dt><i>index</i><dd>
<p>Index is used to create skip list indexes on fields in a table, which can be used to greatly speed up certain types of searches.</p>
<pre>
x index create foo 24
</pre>
<p>...creates a skip list index on field "foo" and sets it to for an optimal size of 2^24 rows. The size value is optional. (How this works will be improved/altered in a subsequent release.) It will index all existing rows in the table and any future rows that are added. Also if a <i>set</i>, <i>read_tabsep</i>, etc, causes a row's indexed value to change, its index will be updated.</p>
<p>If there is already an index present on that field, does nothing.</p>
<pre>
x index drop foo
</pre>
<p>....drops the skip list on field "foo." if there is no such index, does nothing.</p>
<pre>
x index dump foo
</pre>
<p>...dumps the skip list for field "foo". This can be useful to help understand how they work and possibly to look for problems.</p>
<pre>
x index count foo
</pre>
<p>...returns a count of the skip list for field "foo". This number should always match the row count of the table (x count). If it doesn't, there's a bug in index handling.</p>
<pre>x index span foo</pre>
<p>...returns a list containing the lexically lowest entry and the lexically highest entry in the index. If there are no rows in the table, an empty list is returned.</p>
<pre>
x index indexable
</pre>
<p>...returns a (potentially empty) list of all of the field names that can have indexes created for them. Fields must be explicitly defined as indexable when the field is created with <tt>indexed 1</tt> arguments. (This keeps us from incurring a lot of overhead creating various things to be ready to index any field for fields that just couldn't ever reasonably be used as an index anyway.</p>
<pre>
x index indexed
</pre>
<p>...returns a (potentially empty) list of all of the field names in table x that current have an index in existence for them, meaning that index create has been invoked on that field.</p>
</dl>
</body>
</html>
